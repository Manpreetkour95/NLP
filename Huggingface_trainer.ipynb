{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWB1xwndQcLao25DRBic0+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manpreetkour95/NLP/blob/main/Huggingface_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6RzxNmE2Sqt8"
      },
      "outputs": [],
      "source": [
        "#pip install transformers, datasets\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    'text': ['This is a positive review.', \"I didn't like this product.\", 'The weather is great today.', 'The movie was amazing!', 'I feel sad right now.', 'The food at the restaurant was delicious.', 'The service was terrible.', 'I love my new phone!', 'This book is boring.', 'The trip was fantastic.', \"I can't believe I won!\", 'The traffic is horrible.', 'The concert was awesome.', \"I'm really happy today.\", 'The hotel room was clean and comfortable.', 'The test was difficult.', 'The customer service was excellent.', 'I hate waiting in line.', 'The game was fun to play.', \"I'm so excited about the party tonight!\", \"I can't believe I won!\", 'The traffic is horrible.', 'The concert was awesome.', \"I'm really happy today.\", 'The hotel room was clean and comfortable.', 'The test was difficult.', 'The customer service was excellent.', 'I hate waiting in line.', 'The game was fun to play.', \"I'm so excited about the party tonight!I can't believe I won!\", 'The traffic is horrible.', 'The concert was awesome.', \"I'm really happy today.\", 'The hotel room was clean and comfortable.', 'The test was difficult.', 'The customer service was excellent.', 'I hate waiting in line.', 'The game was fun to play.', \"I'm so excited about the party tonight!\", \"I can't believe I won!\", 'The traffic is horrible.', 'The concert was awesome.', \"I'm really happy today.\", 'The hotel room was clean and comfortable.', 'The test was difficult.', 'The customer service was excellent.', 'I hate waiting in line.', 'The game was fun to play.', \"I'm so excited about the party tonight!\", \"I'm so excited\"],\n",
        "\n",
        "    'label': ['positive', 'negative', 'positive', 'positive', 'negative', 'positive', 'negative', 'neutral', 'negative', 'positive', 'positive', 'negative', 'positive', 'positive', 'neutral', 'negative', 'positive', 'negative', 'neutral', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'positive', 'positive', 'negative', 'positive', 'negative', 'positive']\n",
        "\n",
        "}\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "unique_labels = df['label'].unique()\n",
        "\n",
        "# Create a dictionary to map each label to a unique ID\n",
        "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "df['label']=[label2id[label] for label in df.label]\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "\n",
        "# Second split: Split the remaining data into validation and test sets\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "#np.save(\"train_ids.npy\", np.array(train_df.id))\n",
        "#np.save(\"test_ids.npy\", np.array(test_df.id))\n",
        "#np.save(\"tval_ids.npy\", np.array(val_df.id))\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import warnings\n",
        "import re\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, AdamW\n",
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "#accuracy=evaluate.load('accuracy')\n",
        "def prepare_dataset(df):\n",
        "  #df['text']=df['text']\n",
        "  df['label']=df['label'].replace(label2id.keys(),label2id.values())\n",
        "\n",
        "  dataset=DatasetDict()\n",
        "  dataset['train']=Dataset.from_pandas(train_df[['text','label']].reset_index(drop=True))\n",
        "\n",
        "  dataset['val']=Dataset.from_pandas(val_df[['text','label']].reset_index(drop=True))\n",
        "  dataset['test']=Dataset.from_pandas(test_df[['text','label']].reset_index(drop=True))\n",
        "  return dataset\n",
        "\n",
        "\n",
        "# Tokenize the data\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "#model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id),id2label={y:x for x,y in label2id.items()},label2id=label2id)\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label2id), id2label={y:x for x,y in label2id.items()}, label2id=label2id)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "dataset=prepare_dataset(df)\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Define the TrainingArguments and Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='models/'+model_name,\n",
        "    learning_rate=2e-5,\n",
        "    save_strategy='epoch',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    num_train_epochs=4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=16,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=500)\n",
        "\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        ")\n",
        "\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "trainer.save_model('models/'+model_name)\n",
        "\n",
        "#os.makedirs(os.path.join('numpy_dump',model_name), exit_ok=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FCpgAPFBxW6E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}