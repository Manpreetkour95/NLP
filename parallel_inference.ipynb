{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSGdU7j00NrxfGconpd1bn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Manpreetkour95/NLP/blob/main/parallel_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGW-O40CThb4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import concurrent.futures\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
        "def perform_parallel_inference(texts, num_cores=8):\n",
        "\n",
        "\n",
        "    # Function to perform inference on a single text\n",
        "    def predict(text):\n",
        "        return model.encode(text)\n",
        "\n",
        "    # Split the input texts into chunks for parallel processing\n",
        "    chunk_size = len(texts) // num_cores\n",
        "    text_chunks = [texts[i:i+chunk_size] for i in range(0, len(texts), chunk_size)]\n",
        "\n",
        "    # Perform parallel inference\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=num_cores) as executor:\n",
        "        embeddings = list(executor.map(predict, text_chunks))\n",
        "\n",
        "    # Combine results from different threads\n",
        "    embeddings = np.concatenate(embeddings, axis=0)\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "# Load the SentenceTransformer model (replace with your BERT model)\n",
        "# model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "# Sample input data\n",
        "texts = [\"This is a sample sentence.\", \"Another example sentence.\", \"BERT model parallelization.\",\n",
        "         \"This is a sample sentence.\", \"Another example sentence.\", \"BERT model parallelization.\",\n",
        "         \"This is a sample sentence.\", \"Another example sentence.\", \"BERT model parallelization.\",\n",
        "         \"This is a sample sentence.\", \"Another example sentence.\", \"BERT model parallelization.\"]*8\n",
        "\n",
        "# Perform parallel inference and print the embeddings\n",
        "embeddings = perform_parallel_inference(texts)\n",
        "print(embeddings)"
      ],
      "metadata": {
        "id": "9Gk6l_FLTzxN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, BackgroundTasks, HTTPException\n",
        "from fastapi.responses import JSONResponse\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "import torch.nn.functional as F\n",
        "import asyncio\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('optimum/all-MiniLM-L6-v2')\n",
        "\n",
        "# Load the ONNX model\n",
        "onnx_model_path = \"onnx/model.onnx\"\n",
        "\n",
        "# note: for bool type options in python API, set them as False/True\n",
        "providers = [\n",
        "    ('TensorrtExecutionProvider', {\n",
        "        'device_id': 0,\n",
        "        'trt_max_workspace_size': 2147483648,\n",
        "        'trt_fp16_enable': True,\n",
        "    }),\n",
        "    ('CUDAExecutionProvider', {\n",
        "        'device_id': 0,\n",
        "        'arena_extend_strategy': 'kNextPowerOfTwo',\n",
        "        'gpu_mem_limit': 2 * 1024 * 1024 * 1024,\n",
        "        'cudnn_conv_algo_search': 'EXHAUSTIVE',\n",
        "        'do_copy_in_default_stream': True,\n",
        "    })\n",
        "]\n",
        "\n",
        "sess_opt = ort.SessionOptions()\n",
        "ort_session = ort.InferenceSession(onnx_model_path, sess_options=sess_opt, providers=providers)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "class InputText(BaseModel):\n",
        "    text_list: List[str]\n",
        "\n",
        "async def make_predictions_onnx(input_ids,attention_mask,token_type_ids):\n",
        "    ort_inputs = {\n",
        "        'input_ids': input_ids.cpu().numpy(),\n",
        "        'attention_mask': attention_mask.cpu().numpy(),\n",
        "        \"token_type_ids\":token_type_ids.cpu().numpy()\n",
        "    }\n",
        "    ort_outputs = ort_session.run(None, ort_inputs)\n",
        "    return torch.tensor(ort_outputs[0]), attention_mask.cpu().numpy()\n",
        "\n",
        "\n",
        "async def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "\n",
        "async def process_text(text):\n",
        "        encoded_input = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
        "        model_output=await make_predictions_onnx(encoded_input['input_ids'],encoded_input['attention_mask'],encoded_input['token_type_ids'])\n",
        "        sentence_embeddings = await mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "        sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "        return sentence_embeddings.tolist()\n",
        "\n",
        "import time\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "async def predict_texts(input_data: InputText):\n",
        "    input_texts = input_data.text_list\n",
        "    print(input_texts)\n",
        "\n",
        "    start_time = time.time()  # Record the start time\n",
        "\n",
        "    tasks = [process_text(text) for text in input_texts]\n",
        "    print(\"here\")\n",
        "    results = await asyncio.gather(*tasks)\n",
        "\n",
        "    elapsed_time = (time.time() - start_time) * 1000  # Calculate overall elapsed time in milliseconds\n",
        "    print(f\"Overall time taken: {elapsed_time:.2f} ms\")\n",
        "    return results\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "AGQqnPu1UA3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "From nvcr.io/nvidia/tensorrt:22.08-py3\n",
        "\n",
        "COPY ./app.py ./\n",
        "\n",
        "COPY ./onnx ./onnx\n",
        "\n",
        "RUN pip install onnxruntime_gpu==1.12.0 uvicorn transformers fastapi"
      ],
      "metadata": {
        "id": "dHSixqcPUDSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import torch\n",
        "# Replace this URL with the appropriate endpoint if you are running on a different host or port\n",
        "url = \"http://172.17.0.2:8000/predict/\"\n",
        "\n",
        "# Example input data\n",
        "input_data = {\"text_list\": [\"My name is MK\"]}\n",
        "\n",
        "# Send a POST request to the /predict/ endpoint\n",
        "response = requests.post(url, json=input_data)\n",
        "\n",
        "# Print the response\n",
        "print(response.status_code)\n",
        "if response.status_code == 200:\n",
        "    for text in response.json():\n",
        "        print(\"here\")\n",
        "        print(torch.tensor(text))"
      ],
      "metadata": {
        "id": "8-LXe0wXUOJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from io import BytesIO\n",
        "from io import StringIO\n",
        "import boto3\n",
        "from boto3.session import Session\n",
        "import pandas as pd\n",
        "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
        "import time\n",
        "from opensearchpy.helpers import parallel_bulk\n",
        "\n",
        "client_type = 's3'\n",
        "account_id = '757738260077'\n",
        "role = 'DeveloperRole'\n",
        "region = 'us-east-1'\n",
        "index_name = 'vector_search_22feb'\n",
        "\n",
        "def get_sts_session(account_id, role, region):\n",
        "    sts_client = boto3.client('sts')\n",
        "    role_arn = f'arn:aws:iam::{account_id}:role/{role}'\n",
        "    role_session_name = f'{role}-Session'\n",
        "    response = sts_client.assume_role(RoleArn=role_arn, RoleSessionName=role_session_name)\n",
        "    return Session(\n",
        "        aws_access_key_id=response['Credentials']['AccessKeyId'],\n",
        "        aws_secret_access_key=response['Credentials']['SecretAccessKey'],\n",
        "        aws_session_token=response['Credentials']['SessionToken'],\n",
        "        region_name=region\n",
        "    )\n",
        "\n",
        "def get_s3_objects(s3_client, s3_bucket, s3_directory):\n",
        "    response = s3_client.list_objects(Bucket=s3_bucket, Prefix=s3_directory)\n",
        "    return [obj['Key'] for obj in response.get('Contents', [])]\n",
        "\n",
        "def initialize_opensearch(aos_host):\n",
        "    return OpenSearch(\n",
        "        hosts=[{'host': aos_host, 'port': 443}],\n",
        "        use_ssl=True,\n",
        "        verify_certs=True,\n",
        "        connection_class=RequestsHttpConnection,\n",
        "        timeout=90\n",
        "    )\n",
        "\n",
        "def generate_data(text_data, vector_sentences):\n",
        "    for text, vector in zip(text_data, vector_sentences):\n",
        "        yield {\n",
        "            \"_op_type\": \"index\",\n",
        "            \"_index\": index_name,\n",
        "            \"_source\": {\"nameadd_vector\": vector, \"nameadd\": text}\n",
        "        }\n",
        "\n",
        "def parallel_bulk_indexing(aos_client, text_data, vector_sentences):\n",
        "    for success, info in parallel_bulk(\n",
        "        client=aos_client,\n",
        "        index=index_name,\n",
        "        actions=generate_data(text_data, vector_sentences),\n",
        "        timeout=6,\n",
        "        thread_count=4,\n",
        "        chunk_size=5000,\n",
        "        raise_on_error=True,\n",
        "        raise_on_exception=True\n",
        "    ):\n",
        "        if not success:\n",
        "            print('A document failed:', info)\n",
        "\n",
        "# Your provided code snippet\n",
        "s3_bucket = 'iwave-datalake-dev'\n",
        "s3_directory = 'prebuilt_profiles/master_list/master_20231101'\n",
        "s3_session = get_sts_session(account_id, role, region)\n",
        "s3_client = s3_session.client(client_type)\n",
        "list_key = get_s3_objects(s3_client, s3_bucket, s3_directory)\n",
        "#Skipping files which does not have vemb\n",
        "list_key = list_key[0:361]+list_key[362:488]+list_key[489::]\n",
        "print(len(list_key))\n",
        "print(list_key[-1])\n",
        "\n",
        "s3_directory = 'Vector-embeddings/vemb_20231101'\n",
        "list_key1 = get_s3_objects(s3_client, s3_bucket, s3_directory)\n",
        "print(len(list_key1))\n",
        "print(list_key1[-1])\n",
        "\n",
        "# Initializing OpenSearch client\n",
        "aos_host_sandbox = 'vpc-iwave-os-cluster-prebuilt-tioaie3iiezmt3w3jym2ov7cry.us-east-1.es.amazonaws.com'\n",
        "aos_client = initialize_opensearch(aos_host_sandbox)\n",
        "# Your provided code snippet with parallel bulk indexing\n",
        "#num_threads = 4  # Adjust as needed\n",
        "for key, key1 in zip(list_key[1:2], list_key1[1:2]):\n",
        "    print(key)\n",
        "    print(key1)\n",
        "    start_time = time.time()\n",
        "    df = pd.DataFrame()\n",
        "    response = s3_client.get_object(Bucket=s3_bucket, Key=key)\n",
        "    data = response['Body'].read().decode('utf-8')\n",
        "\n",
        "    data_io = StringIO(data)\n",
        "    df = pd.read_csv(data_io, sep='|')\n",
        "    df = df.fillna('')\n",
        "    df['nameadd'] = (df['Prefix'] + ' ' + df['FirstName'] + ' ' + df['MiddleName_Initial'] + ' ' +\n",
        "                     df['LastName'] + ' ' + df['Address1'] + ' ' + df['City'] + ' ' +\n",
        "                     df['State_Province'] + ' ' + df['Country'] + ' ' + df['ZIP_PostalCode'])\n",
        "    text_data = df['nameadd'].to_list()\n",
        "\n",
        "    response = s3_client.get_object(Bucket=s3_bucket, Key=key1)\n",
        "    data = response['Body'].read()\n",
        "    bytes_io = BytesIO(data)\n",
        "    vector_sentences = np.load(bytes_io)\n",
        "\n",
        "    print('adding')\n",
        "    parallel_bulk_indexing(aos_client, text_data, vector_sentences)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Time elapsed: {elapsed_time} seconds\")"
      ],
      "metadata": {
        "id": "7JETNOmsUa3D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}